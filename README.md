# ğŸ§  MedGemma-AD: Multimodal Alzheimerâ€™s Detection & Monitoring

**AI-Powered Early Screening using MedGemma, Speech Analysis & Facial Biomarkers**

[![Built with v0](https://img.shields.io/badge/Built%20with-v0.app-black?style=for-the-badge)](https://v0.app/chat/ldw89y3h9oT)

---

## ğŸš€ Overview

**MedGemma-AD** is a lightweight, multimodal Alzheimerâ€™s early-detection MVP designed for accessibility in both urban and rural healthcare environments.

The system integrates:

* **MedGemma** for medical multimodal reasoning (MRI, PET, cognitive reports)
* **Speech-based impairment analysis** (wav2vec2 / MFCC-based features)
* **Facial micro-expression tracking** (YOLO + emotion embeddings)
* **Fusion-based Alzheimerâ€™s risk prediction**
* **Explainable AI** using SHAP & Grad-CAM
* **Offline/Edge Mode** with ONNX-quantized models + local SQLite storage
* **Caregiver/Doctor dashboard** built using *v0.app*

This repository mirrors your **live deployed app**, automatically synced.

---

## ğŸ§© Problem Statement

Early detection of Alzheimerâ€™s disease is limited by:

* High diagnostic cost of MRI/PET
* Shortage of neurologists
* Lack of screening infrastructure in rural areas
* Delayed identification of mild cognitive impairment

A **low-cost, AI-assisted screening tool** is needed that supports **offline use**, **multimodal inputs**, and **progress monitoring**.

---

## ğŸ’¡ Solution

Our approach combines **MedGemma** with cognitive biomarkers to build an affordable Alzheimer's detection system.

### ğŸ”¬ Multimodal Inputs

* **Medical Imaging:** MRI/PET processed via MedGemma
* **Speech:** pause rate, fluency, acoustic biomarkers
* **Facial Expressions:** expressivity & micro-movement via YOLO
* **Cognitive Text:** MMSE/MoCA answers encoded via MedGemma

### ğŸ§  AI Pipeline

1. Preprocess speech, facial frames, imaging, cognitive text
2. Generate MedGemma multimodal embeddings
3. Extract speech & facial features
4. Fuse all modalities
5. Predict Alzheimerâ€™s stage:

   * Normal
   * Mild Cognitive Impairment (MCI)
   * Early Alzheimerâ€™s
6. Provide explainability via SHAP + Grad-CAM

### ğŸ“Š Dashboard

* Risk score visualization
* Longitudinal decline tracking
* AI-generated medical insights
* Local patient database (SQLite)
* Simple UI for low-resource healthcare settings

### ğŸ–¥ï¸ Edge/Offline Mode

* Models exported as **ONNX/TFLite**
* Runs locally on laptops / Jetson Nano
* No internet required
* Local encryption & consent-based data handling

---

## ğŸ—ï¸ Architecture Summary

```
User Input â†’ MRI/PET | Speech | Facial Video | Cognitive Text
       â†“
Preprocessing
       â†“
MedGemma (image/text) + Speech Model + Facial Model
       â†“
Multimodal Fusion Network
       â†“
AD Risk Prediction + Progress Tracking
       â†“
Explainability (SHAP / Grad-CAM)
       â†“
Dashboard Interface
```

---

## ğŸ› ï¸ Tech Stack

### AI / ML

* MedGemma (Google)
* PyTorch / ONNX Runtime
* YOLOv8
* wav2vec2 / torchaudio

### Backend

* Python
* FastAPI / Flask
* SQLite (local)

### Frontend

* v0.app
* Optional: Gradio/Streamlit version

---

## ğŸ”’ Privacy & Ethics

* Local, encrypted storage for offline clinics
* No cloud inference required
* Assistive, not diagnostic
* Explainability included for clinician review
* User consent required for face/voice data

---

## ğŸ”— Deployment Link

Your app is live here:
ğŸ‘‰ **[https://vercel.com/schools-projects/v0-void-hack](https://vercel.com/schools-projects/v0-void-hack)**
*(displayed only as a deployment URL, with no platform mention)*

---

## ğŸ“‚ Continue Building

Edit your interface or workflows at:
ğŸ‘‰ **[https://v0.app/chat/ldw89y3h9oT](https://v0.app/chat/ldw89y3h9oT)**



Just say the word.
